{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Regular Expressions\n",
    "Task 1: HTML Cleaning (30 marks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AIR6001 AI and Applications / MDS6105 Advanced AI\n",
      "\n",
      "AIR6001 AI and Applications\n",
      "MDS6105 Advanced AI\n",
      "\n",
      "Spring 2023\n",
      "\n",
      "This course is more relevant than ever given the recent surge in Artificial Intelligence technologies.\n",
      "It introduces the fundamental concepts, history, and advanced technologies of AI in various applications, \n",
      "providing students with a comprehensive understanding of this rapidly evolving field. \n",
      "The course covers basic concepts of AI, such as intelligent agents, problem-solving, \n",
      "search, and first-order logic. It also delves into fundamental AI technologies, including regression,\n",
      "pattern recognition, sequential modeling, data mining, deep learning, and supervised modeling. \n",
      "Additionally, the course explores technologies used in various AI applications, such as anomaly detection, \n",
      "edge AI, image processing, speech processing, natural language processing (including machine translation \n",
      "and dialogue modeling), scientific paper analysis, AI in healthcare, autonomous driving, and DNN interpretability and bias.\n",
      "\n",
      "Teaching team\n",
      "\n",
      "Instructor Satoshi Nakamura\n",
      "\n",
      "TA Xi Chen\n",
      "\n",
      "Logistics\n",
      "\n",
      "Lectures: are on Tuesday/Thursday 1:30PM - 3:00PM in TXB201(TBD). Note: lectures will be remote\n",
      "for the first two weeks, and hybrid afterwards. The Zoom link is posted on BB.\n",
      "Office hours \n",
      "\n",
      "Prof. Nakamura: Thu 2:30-3:30 PM. Teaching Complex 704\n",
      "TA. Xi Chen: Wed 7-9PM. Teaching Complex 703, Seat No.7\n",
      "\n",
      "Contact: If you have any question, please reach out to us via email or post it to BB.\n",
      "\n",
      "Course Information\n",
      "\n",
      "This course is designed for students interested in artificial intelligence. The first half mainly covers \n",
      "the fundamentals of artificial intelligence, and the second half focuses on the applications of artificial \n",
      "intelligence in various fields.\n",
      "\n",
      "In particular, the topics include:\n",
      "\n",
      "AI History\n",
      "Intelligent Agents\n",
      "Deep Learning\n",
      "Image Processing\n",
      "Natural Language Processing\n",
      "Speech Processing\n",
      "Diallogue\n",
      "AI applications in Health and Self-driving Car, etc.\n",
      "\n",
      "Prerequisites\n",
      "\n",
      "Proficiency in LaTex: All the reports need to be written by using LaTex. A template will be\n",
      "provided. If you are not familiar with LaTex, please learn from the tutorial in advance.\n",
      "\n",
      "Proficiency in GitHub: All the source codes need to be submitted in GitHub. \n",
      "Proficiency in Python: All the assignments will be in Python (using Numpy and PyTorch). \n",
      "Basic machine learning knowledge: It is possible to take this course without any machine\n",
      "learning knowledge, however, the course will be easier if you have foundations of machine learning.\n",
      "\n",
      "Basic Concepts of probability: It will be easier for you to understand some lectures if you\n",
      "know basics of probability. \n",
      "\n",
      "Textbooks\n",
      "\n",
      "Recommended Books:\n",
      "\n",
      "Artificial Intelligence: A Modern Approach 4th Edition\n",
      ", by Stuart Russell and Peter Norvig\n",
      "\n",
      "Grading Policy (AIR6001/MDS6105)\n",
      "Assignments (30%)\n",
      "\n",
      "Assignment 1 (10%): Lecture 1 - Lecture 6\n",
      "Assignment 2 (10%): Lecture 7 - Lecture 11\n",
      "Assignment 3 (10%): Lecture 12 - Lecture 26\n",
      "\n",
      "Midterm exam (30%)\n",
      "(TBD) We will have a mid-term exam after lecture 14. The scope of the mid-term exam is from lecture 1 to\n",
      "lecture 14. \n",
      "\n",
      "Final exam (35%)\n",
      "(TBD) We will have a final exam on TBD. The scope of the final exam is from lecture 1 to lecture 28.\n",
      "\n",
      "Participation (5%)\n",
      "(TBD)Here are some ways to earn the participation credit, which is capped at 5%. \n",
      "\n",
      "Course and Teaching Evaluation (CTE): The school will send requests for CTE to all students.\n",
      "The CTE is worth 1% credit.\n",
      "\n",
      "Late Policy\n",
      "The penalty is 0.5% off the final course grade for each late day.\n",
      "\n",
      "Schedule\n",
      "\n",
      "Date\n",
      "Lecture Description\n",
      "Readings\n",
      "Lecture Note\n",
      "Events/Deadlines\n",
      "\n",
      "Sept 3\n",
      "Lecture 1: Course Introduction/AI Introduction\n",
      "\n",
      "[Slides]\n",
      "\n",
      "Sept 5\n",
      "Lecture 2: AI history/Inteligent agent\n",
      "\n",
      "[Slides]\n",
      "\n",
      "Sept 10\n",
      "Lecture 3: AI Solving Problems by Searching\n",
      "\n",
      "Sept 12\n",
      "Lecture 4: AI Informed Search\n",
      "\n",
      "Sept 19\n",
      "Lecture 5: AI in Constraint satisfaction Problems\n",
      "\n",
      "Sept 24\n",
      "Lecture 6: AI in Logical Agent and 1st order logic\n",
      "\n",
      "Assignment 1 out \n",
      "\n",
      "Sept 26\n",
      "Lecture 7: AI Regression\n",
      "\n",
      "Oct 8\n",
      "Lecture 8: AI Pattern Recognition\n",
      "\n",
      "Oct 10\n",
      "Lecture 9: AI Sequence modeling\n",
      "\n",
      "Oct 15\n",
      "Lecture 10: AI in Data mining\n",
      "\n",
      "Assignment 2 out \n",
      "\n",
      "Oct 17\n",
      "Lecture 11: AI in Deep Learning Fundamentals 1\n",
      "\n",
      "Oct 22\n",
      "Lecture 12: AI in Deep Learning Fundamentals 2\n",
      "\n",
      "Oct 24\n",
      "Lecture 13: AI Self-supervised modeling\n",
      "\n",
      "Oct 29\n",
      "Midterm exam\n",
      "\n",
      "Oct 31\n",
      "Lecture 14: AI in Anormaly 1\n",
      "\n",
      "Assignment 3 out \n",
      "\n",
      "Nov 5\n",
      "Lecture 15: AI in Anormaly 2\n",
      "\n",
      "Nov 7\n",
      "Lecture 16: Edge AI\n",
      "\n",
      "Nov 12\n",
      "Lecture 17: AI Image Processing\n",
      "\n",
      "Nov 14\n",
      "Lecture 18: AI in Speech Processing\n",
      "\n",
      "Nov 19\n",
      "Lecture 19: AI in Natural Language Processing\n",
      "\n",
      "Nov 21\n",
      "Lecture 20: AI in Machine Translation 1\n",
      "\n",
      "Nov 26\n",
      "Lecture 21: AI in Machine Translation 2\n",
      "\n",
      "Nov 28\n",
      "Lecture 22: AI in Dialogue System\n",
      "\n",
      "Dec 3\n",
      "no class (SLT2024)\n",
      "\n",
      "Dec 5\n",
      "Lecture 23: AI in Health, Medical Image Processing I\n",
      "\n",
      "Dec 10\n",
      "Lecture 24: AI in Self-driving Car\n",
      "\n",
      "Dec 12\n",
      "Lecture 25: AI in Interpretability and Ethics\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "def clean_html(input):\n",
    "    with open(input, 'r', encoding='utf-8') as file:\n",
    "        html_content = file.read()\n",
    "\n",
    "    # Extract content within the <title> tag\n",
    "    title_match = re.search(r'<title>(.*?)</title>', html_content, re.DOTALL)\n",
    "    title_content = title_match.group(1).strip() if title_match else \"\"\n",
    "\n",
    "    # Remove HTML comments\n",
    "    html_content = re.sub(r'<!--.*?-->', '', html_content, flags=re.DOTALL)\n",
    "\n",
    "    # Remove all <head> and <style> tags and their content, keeping <title>\n",
    "    html_content = re.sub(r'<head>.*?</head>', '', html_content, flags=re.DOTALL)\n",
    "    html_content = re.sub(r'<style.*?>.*?</style>', '', html_content, flags=re.DOTALL)\n",
    "\n",
    "    # Remove all <script> tags and their content\n",
    "    html_content = re.sub(r'<script.*?>.*?</script>', '', html_content, flags=re.DOTALL)\n",
    "\n",
    "    # Remove all HTML tags, keeping only the content\n",
    "    html_content = re.sub(r'<[^>]+>', '', html_content)\n",
    "\n",
    "    # Remove extra indentation and whitespace characters (including extra line breaks)\n",
    "    html_content = re.sub(r'[ \\t]+', ' ', html_content)  # Replace extra spaces\n",
    "    html_content = re.sub(r'\\n\\s*\\n', '\\n\\n', html_content)  # Merge multiple blank lines into one\n",
    "\n",
    "    # Keep necessary blank lines\n",
    "    html_content = re.sub(r'\\n{3,}', '\\n\\n', html_content)\n",
    "\n",
    "    # Ensure no line starts with a space\n",
    "    html_content = '\\n'.join(line.lstrip() for line in html_content.splitlines())\n",
    "\n",
    "    # Add the <title> content at the top and ensure there is a blank line between it and the body\n",
    "    if title_content:\n",
    "        html_content = title_content + \"\\n\\n\" + html_content.strip()\n",
    "\n",
    "    # Output the processed content directly to the console\n",
    "    print(html_content.strip())\n",
    "\n",
    "\n",
    "# Call the function to process the HTML file and output to console\n",
    "input = '/Users/mengrui/Desktop/input.html'\n",
    "clean_html(input)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Byte-Pair Encoding\n",
    "Task 2: Implement Byte-Pair Encoding (BPE) (30 marks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Implement a Byte-Pair Encoding (BPE) algorithm to learn subword tokens. Val- idate your implementation with the following setup. In every iteration, track the tokens with the most frequency and the occurrence.\n",
    "• The text is “aaabdaaababc aa”\n",
    "• The number of merges k = 2\n",
    "• the expected tokenization output is “{aa}{ab}d{aa}{ab}{ab}c {aa}”’, where ‘{’ and ‘}’ indicate the new tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1: Merging pair ('a', 'a') with frequency 7\n",
      "Iteration 2: Merging pair ('a', 'b') with frequency 3\n",
      "\n",
      "Final tokenized output:\n",
      "{aa}{ab}d{aa}{ab}{ab}c {aa}\n"
     ]
    }
   ],
   "source": [
    "import collections\n",
    "\n",
    "# sub-word BPE algorithm\n",
    "def get_stats(vocab):\n",
    "    \n",
    "    pairs = collections.defaultdict(int)\n",
    "    for word, freq in vocab.items():\n",
    "        symbols = word.split()\n",
    "        for i in range(len(symbols) - 1):\n",
    "            pairs[(symbols[i], symbols[i + 1])] += freq\n",
    "    return pairs\n",
    "\n",
    "def merge_vocab(pair, vocab):\n",
    "    # merge the most frequent charcter pairs\n",
    "    bigram = ' '.join(pair)\n",
    "    new_vocab = {}\n",
    "    for word in vocab:\n",
    "        new_word = word.replace(bigram, ''.join(pair))\n",
    "        new_vocab[new_word] = vocab[word]\n",
    "    return new_vocab\n",
    "\n",
    "def bpe(vocab, num_merges):\n",
    "    # do the specific merge times\n",
    "    merges = []\n",
    "    for i in range(num_merges):\n",
    "        pairs = get_stats(vocab)\n",
    "        if not pairs:\n",
    "            break\n",
    "        # find the most frequent charcter pairs\n",
    "        best = max(pairs, key=pairs.get)\n",
    "        merges.append(best)\n",
    "        print(f\"Iteration {i + 1}: Merging pair {best} with frequency {pairs[best]}\")\n",
    "        vocab = merge_vocab(best, vocab)\n",
    "    return vocab, merges\n",
    "\n",
    "# enter the text\n",
    "text = \"aaabdaaababc aa\"\n",
    "# create a vocab to seperate words, and make merge procudure more conveient\n",
    "vocab = {' '.join(word): text.count(word) for word in text.split()}\n",
    "\n",
    "# do the BPE procedure, number of merge times is 2\n",
    "final_vocab, merges = bpe(vocab, num_merges=2)\n",
    "\n",
    "# print out the last form of token\n",
    "output = list(text)\n",
    "for merge in merges:\n",
    "    bigram = ''.join(merge)\n",
    "    new_token = '{' + bigram + '}'\n",
    "    text_str = ''.join(output)\n",
    "    text_str = text_str.replace(bigram, new_token)\n",
    "    output = list(text_str)\n",
    "\n",
    "final_output = ''.join(output)\n",
    "\n",
    "print(\"\\nFinal tokenized output:\")\n",
    "print(final_output)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Apply BPE to the vocabulary of output.txt. In every iteration, track the tokens with the most frequency and the occurrence. Set the number of merges k = 20.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1: Merging pair ('i', 'n') with frequency 185\n",
      "Iteration 2: Merging pair ('o', 'n') with frequency 79\n",
      "Iteration 3: Merging pair ('r', 'e') with frequency 70\n",
      "Iteration 4: Merging pair ('e', 'c') with frequency 61\n",
      "Iteration 5: Merging pair ('a', 't') with frequency 60\n",
      "Iteration 6: Merging pair ('in', 'g') with frequency 54\n",
      "Iteration 7: Merging pair ('e', 'n') with frequency 53\n",
      "Iteration 8: Merging pair ('t', 'u') with frequency 50\n",
      "Iteration 9: Merging pair ('e', 'a') with frequency 48\n",
      "Iteration 10: Merging pair ('A', 'I') with frequency 47\n",
      "Iteration 11: Merging pair ('o', 'r') with frequency 46\n",
      "Iteration 12: Merging pair ('ec', 'tu') with frequency 44\n",
      "Iteration 13: Merging pair ('ectu', 're') with frequency 44\n",
      "Iteration 14: Merging pair ('a', 'l') with frequency 38\n",
      "Iteration 15: Merging pair ('i', 't') with frequency 38\n",
      "Iteration 16: Merging pair ('a', 'n') with frequency 37\n",
      "Iteration 17: Merging pair ('i', 'c') with frequency 37\n",
      "Iteration 18: Merging pair ('i', 's') with frequency 36\n",
      "Iteration 19: Merging pair ('L', 'ecture') with frequency 35\n",
      "Iteration 20: Merging pair ('t', 'e') with frequency 33\n",
      "\n",
      "Final tokenized output:\n",
      "{AI}R6001 {AI} {an}d Appl{ic}{at}i{on}s / MDS6105 Adv{an}ced {AI}\n",
      "\n",
      "{AI}R6001 {AI} {an}d Appl{ic}{at}i{on}s\n",
      "MDS6105 Adv{an}ced {AI}\n",
      "\n",
      "Spr{ing} 2023\n",
      "\n",
      "Th{is} course {is} mo{re} {re}lev{an}t th{an} ever giv{en} the {re}c{en}t surge {in} Artif{ic}i{al} In{te}llig{en}ce t{ec}hnologies.\n",
      "It {in}troduces the fundam{en}t{al} c{on}cepts, h{is}t{or}y, {an}d adv{an}ced t{ec}hnologies of {AI} {in} various appl{ic}{at}i{on}s, \n",
      "provid{ing} s{tu}d{en}ts w{it}h a comp{re}h{en}sive underst{an}d{ing} of th{is} rapidly evolv{ing} field. \n",
      "The course covers bas{ic} c{on}cepts of {AI}, such as {in}{te}llig{en}t ag{en}ts, problem-solv{ing}, \n",
      "s{ea}rch, {an}d first-{or}der log{ic}. It {al}so delves {in}to fundam{en}t{al} {AI} t{ec}hnologies, {in}clud{ing} {re}g{re}ssi{on},\n",
      "p{at}{te}rn {re}cogn{it}i{on}, sequ{en}ti{al} model{ing}, d{at}a m{in}{ing}, deep l{ea}rn{ing}, {an}d superv{is}ed model{ing}. \n",
      "Add{it}i{on}{al}ly, the course explo{re}s t{ec}hnologies used {in} various {AI} appl{ic}{at}i{on}s, such as {an}om{al}y det{ec}ti{on}, \n",
      "edge {AI}, image process{ing}, spe{ec}h process{ing}, n{at}ur{al} l{an}guage process{ing} ({in}clud{ing} mach{in}e tr{an}sl{at}i{on} \n",
      "{an}d di{al}ogue model{ing}), sci{en}tif{ic} paper {an}{al}ys{is}, {AI} {in} h{ea}lthca{re}, aut{on}omous driv{ing}, {an}d DNN {in}{te}rp{re}tabil{it}y {an}d bias.\n",
      "\n",
      "T{ea}ch{ing} t{ea}m\n",
      "\n",
      "Instruct{or} S{at}oshi Nakamura\n",
      "\n",
      "TA Xi Ch{en}\n",
      "\n",
      "Log{is}t{ic}s\n",
      "\n",
      "{Lecture}s: a{re} {on} Tuesday/Thursday 1:30PM - 3:00PM {in} TXB201(TBD). No{te}: l{ecture}s will be {re}mo{te}\n",
      "f{or} the first two weeks, {an}d hybrid af{te}rwards. The Zoom l{in}k {is} pos{te}d {on} BB.\n",
      "Off{ic}e hours \n",
      "\n",
      "Prof. Nakamura: Thu 2:30-3:30 PM. T{ea}ch{ing} Complex 704\n",
      "TA. Xi Ch{en}: Wed 7-9PM. T{ea}ch{ing} Complex 703, Se{at} No.7\n",
      "\n",
      "C{on}tact: If you have {an}y questi{on}, pl{ea}se {re}ach out to us via email {or} post {it} to BB.\n",
      "\n",
      "Course Inf{or}m{at}i{on}\n",
      "\n",
      "Th{is} course {is} designed f{or} s{tu}d{en}ts {in}{te}{re}s{te}d {in} artif{ic}i{al} {in}{te}llig{en}ce. The first h{al}f ma{in}ly covers \n",
      "the fundam{en}t{al}s of artif{ic}i{al} {in}{te}llig{en}ce, {an}d the s{ec}{on}d h{al}f focuses {on} the appl{ic}{at}i{on}s of artif{ic}i{al} \n",
      "{in}{te}llig{en}ce {in} various fields.\n",
      "\n",
      "In part{ic}ular, the top{ic}s {in}clude:\n",
      "\n",
      "{AI} H{is}t{or}y\n",
      "In{te}llig{en}t Ag{en}ts\n",
      "Deep L{ea}rn{ing}\n",
      "Image Process{ing}\n",
      "N{at}ur{al} L{an}guage Process{ing}\n",
      "Spe{ec}h Process{ing}\n",
      "Di{al}logue\n",
      "{AI} appl{ic}{at}i{on}s {in} H{ea}lth {an}d Self-driv{ing} Car, etc.\n",
      "\n",
      "P{re}{re}qu{is}{it}es\n",
      "\n",
      "Prof{ic}i{en}cy {in} LaTex: All the {re}p{or}ts need to be wr{it}t{en} by us{ing} LaTex. A {te}mpl{at}e will be\n",
      "provided. If you a{re} not familiar w{it}h LaTex, pl{ea}se l{ea}rn from the {tu}t{or}i{al} {in} adv{an}ce.\n",
      "\n",
      "Prof{ic}i{en}cy {in} G{it}Hub: All the source codes need to be subm{it}{te}d {in} G{it}Hub. \n",
      "Prof{ic}i{en}cy {in} Pyth{on}: All the assignm{en}ts will be {in} Pyth{on} (us{ing} Numpy {an}d PyT{or}ch). \n",
      "Bas{ic} mach{in}e l{ea}rn{ing} knowledge: It {is} possible to take th{is} course w{it}hout {an}y mach{in}e\n",
      "l{ea}rn{ing} knowledge, however, the course will be {ea}sier if you have found{at}i{on}s of mach{in}e l{ea}rn{ing}.\n",
      "\n",
      "Bas{ic} C{on}cepts of probabil{it}y: It will be {ea}sier f{or} you to underst{an}d some l{ecture}s if you\n",
      "know bas{ic}s of probabil{it}y. \n",
      "\n",
      "Textbooks\n",
      "\n",
      "R{ec}omm{en}ded Books:\n",
      "\n",
      "Artif{ic}i{al} In{te}llig{en}ce: A Modern Approach 4th Ed{it}i{on}\n",
      ", by S{tu}art Russell {an}d Pe{te}r N{or}vig\n",
      "\n",
      "Grad{ing} Pol{ic}y ({AI}R6001/MDS6105)\n",
      "Assignm{en}ts (30%)\n",
      "\n",
      "Assignm{en}t 1 (10%): {Lecture} 1 - {Lecture} 6\n",
      "Assignm{en}t 2 (10%): {Lecture} 7 - {Lecture} 11\n",
      "Assignm{en}t 3 (10%): {Lecture} 12 - {Lecture} 26\n",
      "\n",
      "Mid{te}rm exam (30%)\n",
      "(TBD) We will have a mid-{te}rm exam af{te}r l{ecture} 14. The scope of the mid-{te}rm exam {is} from l{ecture} 1 to\n",
      "l{ecture} 14. \n",
      "\n",
      "F{in}{al} exam (35%)\n",
      "(TBD) We will have a f{in}{al} exam {on} TBD. The scope of the f{in}{al} exam {is} from l{ecture} 1 to l{ecture} 28.\n",
      "\n",
      "Part{ic}ip{at}i{on} (5%)\n",
      "(TBD)He{re} a{re} some ways to {ea}rn the part{ic}ip{at}i{on} c{re}d{it}, wh{ic}h {is} capped {at} 5%. \n",
      "\n",
      "Course {an}d T{ea}ch{ing} Ev{al}u{at}i{on} (CTE): The school will s{en}d {re}quests f{or} CTE to {al}l s{tu}d{en}ts.\n",
      "The CTE {is} w{or}th 1% c{re}d{it}.\n",
      "\n",
      "L{at}e Pol{ic}y\n",
      "The p{en}{al}ty {is} 0.5% off the f{in}{al} course grade f{or} {ea}ch l{at}e day.\n",
      "\n",
      "Schedule\n",
      "\n",
      "D{at}e\n",
      "{Lecture} Descripti{on}\n",
      "R{ea}d{ing}s\n",
      "{Lecture} No{te}\n",
      "Ev{en}ts/D{ea}dl{in}es\n",
      "\n",
      "Sept 3\n",
      "{Lecture} 1: Course Introducti{on}/{AI} Introducti{on}\n",
      "\n",
      "[Slides]\n",
      "\n",
      "Sept 5\n",
      "{Lecture} 2: {AI} h{is}t{or}y/In{te}lig{en}t ag{en}t\n",
      "\n",
      "[Slides]\n",
      "\n",
      "Sept 10\n",
      "{Lecture} 3: {AI} Solv{ing} Problems by S{ea}rch{ing}\n",
      "\n",
      "Sept 12\n",
      "{Lecture} 4: {AI} Inf{or}med S{ea}rch\n",
      "\n",
      "Sept 19\n",
      "{Lecture} 5: {AI} {in} C{on}stra{in}t s{at}{is}facti{on} Problems\n",
      "\n",
      "Sept 24\n",
      "{Lecture} 6: {AI} {in} Log{ic}{al} Ag{en}t {an}d 1st {or}der log{ic}\n",
      "\n",
      "Assignm{en}t 1 out \n",
      "\n",
      "Sept 26\n",
      "{Lecture} 7: {AI} Reg{re}ssi{on}\n",
      "\n",
      "Oct 8\n",
      "{Lecture} 8: {AI} P{at}{te}rn R{ec}ogn{it}i{on}\n",
      "\n",
      "Oct 10\n",
      "{Lecture} 9: {AI} Sequ{en}ce model{ing}\n",
      "\n",
      "Oct 15\n",
      "{Lecture} 10: {AI} {in} D{at}a m{in}{ing}\n",
      "\n",
      "Assignm{en}t 2 out \n",
      "\n",
      "Oct 17\n",
      "{Lecture} 11: {AI} {in} Deep L{ea}rn{ing} Fundam{en}t{al}s 1\n",
      "\n",
      "Oct 22\n",
      "{Lecture} 12: {AI} {in} Deep L{ea}rn{ing} Fundam{en}t{al}s 2\n",
      "\n",
      "Oct 24\n",
      "{Lecture} 13: {AI} Self-superv{is}ed model{ing}\n",
      "\n",
      "Oct 29\n",
      "Mid{te}rm exam\n",
      "\n",
      "Oct 31\n",
      "{Lecture} 14: {AI} {in} An{or}m{al}y 1\n",
      "\n",
      "Assignm{en}t 3 out \n",
      "\n",
      "Nov 5\n",
      "{Lecture} 15: {AI} {in} An{or}m{al}y 2\n",
      "\n",
      "Nov 7\n",
      "{Lecture} 16: Edge {AI}\n",
      "\n",
      "Nov 12\n",
      "{Lecture} 17: {AI} Image Process{ing}\n",
      "\n",
      "Nov 14\n",
      "{Lecture} 18: {AI} {in} Spe{ec}h Process{ing}\n",
      "\n",
      "Nov 19\n",
      "{Lecture} 19: {AI} {in} N{at}ur{al} L{an}guage Process{ing}\n",
      "\n",
      "Nov 21\n",
      "{Lecture} 20: {AI} {in} Mach{in}e Tr{an}sl{at}i{on} 1\n",
      "\n",
      "Nov 26\n",
      "{Lecture} 21: {AI} {in} Mach{in}e Tr{an}sl{at}i{on} 2\n",
      "\n",
      "Nov 28\n",
      "{Lecture} 22: {AI} {in} Di{al}ogue Sys{te}m\n",
      "\n",
      "D{ec} 3\n",
      "no class (SLT2024)\n",
      "\n",
      "D{ec} 5\n",
      "{Lecture} 23: {AI} {in} H{ea}lth, Med{ic}{al} Image Process{ing} I\n",
      "\n",
      "D{ec} 10\n",
      "{Lecture} 24: {AI} {in} Self-driv{ing} Car\n",
      "\n",
      "D{ec} 12\n",
      "{Lecture} 25: {AI} {in} In{te}rp{re}tabil{it}y {an}d Eth{ic}s\n"
     ]
    }
   ],
   "source": [
    "import collections\n",
    "\n",
    "# sub-word BPE algorithm\n",
    "def get_stats(vocab):\n",
    "    # calculate most frequency char pairs\n",
    "    pairs = collections.defaultdict(int)\n",
    "    for word, freq in vocab.items():\n",
    "        symbols = word.split()\n",
    "        for i in range(len(symbols) - 1):\n",
    "            pairs[(symbols[i], symbols[i + 1])] += freq\n",
    "    return pairs\n",
    "\n",
    "def merge_vocab(pair, vocab):\n",
    "    # merge most frequent char pairs\n",
    "    bigram = ' '.join(pair)\n",
    "    new_vocab = {}\n",
    "    for word in vocab:\n",
    "        new_word = word.replace(bigram, ''.join(pair))\n",
    "        new_vocab[new_word] = vocab[word]\n",
    "    return new_vocab\n",
    "\n",
    "def bpe(vocab, num_merges):\n",
    "    # do k times of merge \n",
    "    merges = []\n",
    "    for i in range(num_merges):\n",
    "        pairs = get_stats(vocab)\n",
    "        if not pairs:\n",
    "            break\n",
    "        # find the most frequent char pairs\n",
    "        best = max(pairs, key=pairs.get)\n",
    "        merges.append(best)\n",
    "        print(f\"Iteration {i + 1}: Merging pair {best} with frequency {pairs[best]}\")\n",
    "        vocab = merge_vocab(best, vocab)\n",
    "    return vocab, merges\n",
    "\n",
    "# enter the file path\n",
    "file_path = '/Users/mengrui/Desktop/output.txt'  \n",
    "\n",
    "# read the file content\n",
    "with open(file_path, 'r') as file:\n",
    "    text = file.read().strip()\n",
    "# create a vocab, seperate every char with a space in case to make merge more convenient\n",
    "vocab = {' '.join(word): text.count(word) for word in text.split()}\n",
    "\n",
    "# do the BPE algorithm with 20 times\n",
    "final_vocab, merges = bpe(vocab, num_merges=20)\n",
    "\n",
    "# print out the final form of token\n",
    "output = list(text)\n",
    "for merge in merges:\n",
    "    tokens_to_merge = ['{' + token + '}' if len(token) > 1 else token for token in merge]\n",
    "    replaced_token = ''.join(tokens_to_merge)\n",
    "    bigram = ''.join(merge)\n",
    "    new_token = '{' + bigram + '}'\n",
    "    text_str = ''.join(output)\n",
    "    text_str = text_str.replace(replaced_token, new_token)\n",
    "    output = list(text_str)\n",
    "\n",
    "final_output = ''.join(output)\n",
    "\n",
    "print(\"\\nFinal tokenized output:\")\n",
    "print(final_output)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Among new tokens in Task 2.2, what are the three longest tokens. Do not list the component of the listed word. For example, if you have two tokens “speech” and “eech”, please only put “speech”. Discuss the reasons why those three tokens were selected."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Answer: The first longest tokens is \"Lecture\", the second longest tokens is \"ing\", while the third longest tokens is in(the highest frequency). The word in is selected probably because it is the most frequently used word pair in this text. And about ing it's a common suffix of adjectives. And \"lecture\" commonly appears in the Schedule part, so that's why these three are the most frequent word."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. Set the number of merges k = 100 and discuss the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1: Merging pair ('i', 'n') with frequency 185\n",
      "Iteration 2: Merging pair ('o', 'n') with frequency 79\n",
      "Iteration 3: Merging pair ('r', 'e') with frequency 70\n",
      "Iteration 4: Merging pair ('e', 'c') with frequency 61\n",
      "Iteration 5: Merging pair ('a', 't') with frequency 60\n",
      "Iteration 6: Merging pair ('in', 'g') with frequency 54\n",
      "Iteration 7: Merging pair ('e', 'n') with frequency 53\n",
      "Iteration 8: Merging pair ('t', 'u') with frequency 50\n",
      "Iteration 9: Merging pair ('e', 'a') with frequency 48\n",
      "Iteration 10: Merging pair ('A', 'I') with frequency 47\n",
      "Iteration 11: Merging pair ('o', 'r') with frequency 46\n",
      "Iteration 12: Merging pair ('ec', 'tu') with frequency 44\n",
      "Iteration 13: Merging pair ('ectu', 're') with frequency 44\n",
      "Iteration 14: Merging pair ('a', 'l') with frequency 38\n",
      "Iteration 15: Merging pair ('i', 't') with frequency 38\n",
      "Iteration 16: Merging pair ('a', 'n') with frequency 37\n",
      "Iteration 17: Merging pair ('i', 'c') with frequency 37\n",
      "Iteration 18: Merging pair ('i', 's') with frequency 36\n",
      "Iteration 19: Merging pair ('L', 'ecture') with frequency 35\n",
      "Iteration 20: Merging pair ('t', 'e') with frequency 33\n",
      "Iteration 21: Merging pair ('en', 't') with frequency 32\n",
      "Iteration 22: Merging pair ('c', 'e') with frequency 31\n",
      "Iteration 23: Merging pair ('r', 'o') with frequency 31\n",
      "Iteration 24: Merging pair ('t', 'h') with frequency 30\n",
      "Iteration 25: Merging pair ('i', 'on') with frequency 29\n",
      "Iteration 26: Merging pair ('o', 'u') with frequency 28\n",
      "Iteration 27: Merging pair ('c', 'h') with frequency 27\n",
      "Iteration 28: Merging pair ('ea', 'r') with frequency 27\n",
      "Iteration 29: Merging pair ('d', 'e') with frequency 26\n",
      "Iteration 30: Merging pair ('s', 's') with frequency 26\n",
      "Iteration 31: Merging pair ('n', 'o') with frequency 25\n",
      "Iteration 32: Merging pair ('I', 'n') with frequency 23\n",
      "Iteration 33: Merging pair ('ear', 'n') with frequency 23\n",
      "Iteration 34: Merging pair ('l', 'l') with frequency 22\n",
      "Iteration 35: Merging pair ('i', 'g') with frequency 22\n",
      "Iteration 36: Merging pair ('an', 'd') with frequency 21\n",
      "Iteration 37: Merging pair ('r', 's') with frequency 21\n",
      "Iteration 38: Merging pair ('t', 'o') with frequency 21\n",
      "Iteration 39: Merging pair ('a', 's') with frequency 19\n",
      "Iteration 40: Merging pair ('1', '0') with frequency 18\n",
      "Iteration 41: Merging pair ('th', 'e') with frequency 17\n",
      "Iteration 42: Merging pair ('a', 'm') with frequency 17\n",
      "Iteration 43: Merging pair ('e', 'd') with frequency 17\n",
      "Iteration 44: Merging pair ('at', 'ion') with frequency 16\n",
      "Iteration 45: Merging pair ('o', 'g') with frequency 16\n",
      "Iteration 46: Merging pair ('o', 'f') with frequency 16\n",
      "Iteration 47: Merging pair ('a', 'r') with frequency 16\n",
      "Iteration 48: Merging pair ('u', 's') with frequency 15\n",
      "Iteration 49: Merging pair ('p', 'l') with frequency 14\n",
      "Iteration 50: Merging pair ('g', 'e') with frequency 14\n",
      "Iteration 51: Merging pair ('i', 'f') with frequency 14\n",
      "Iteration 52: Merging pair ('e', 'x') with frequency 14\n",
      "Iteration 53: Merging pair ('T', 'h') with frequency 13\n",
      "Iteration 54: Merging pair ('e', 's') with frequency 13\n",
      "Iteration 55: Merging pair ('c', 't') with frequency 13\n",
      "Iteration 56: Merging pair ('p', 't') with frequency 12\n",
      "Iteration 57: Merging pair ('te', 'r') with frequency 12\n",
      "Iteration 58: Merging pair ('ro', 'cess') with frequency 12\n",
      "Iteration 59: Merging pair ('rocess', 'ing') with frequency 12\n",
      "Iteration 60: Merging pair ('N', 'o') with frequency 12\n",
      "Iteration 61: Merging pair ('ou', 'rs') with frequency 11\n",
      "Iteration 62: Merging pair ('l', 'earn') with frequency 11\n",
      "Iteration 63: Merging pair ('S', 'e') with frequency 11\n",
      "Iteration 64: Merging pair ('ours', 'e') with frequency 10\n",
      "Iteration 65: Merging pair ('v', 'e') with frequency 10\n",
      "Iteration 66: Merging pair ('ic', 'i') with frequency 10\n",
      "Iteration 67: Merging pair ('te', 'll') with frequency 10\n",
      "Iteration 68: Merging pair ('m', 'a') with frequency 10\n",
      "Iteration 69: Merging pair ('ea', 'ch') with frequency 10\n",
      "Iteration 70: Merging pair ('ig', 'n') with frequency 10\n",
      "Iteration 71: Merging pair ('ation', 's') with frequency 9\n",
      "Iteration 72: Merging pair ('l', 'og') with frequency 9\n",
      "Iteration 73: Merging pair ('u', 'n') with frequency 9\n",
      "Iteration 74: Merging pair ('a', 'p') with frequency 9\n",
      "Iteration 75: Merging pair ('de', 'l') with frequency 9\n",
      "Iteration 76: Merging pair ('m', 'o') with frequency 9\n",
      "Iteration 77: Merging pair ('3', ':') with frequency 9\n",
      "Iteration 78: Merging pair ('f', 'or') with frequency 9\n",
      "Iteration 79: Merging pair ('ign', 'm') with frequency 9\n",
      "Iteration 80: Merging pair ('ignm', 'ent') with frequency 9\n",
      "Iteration 81: Merging pair ('pl', 'ic') with frequency 8\n",
      "Iteration 82: Merging pair ('plic', 'ations') with frequency 8\n",
      "Iteration 83: Merging pair ('tell', 'igen') with frequency 8\n",
      "Iteration 84: Merging pair ('telligen', 'ce') with frequency 8\n",
      "Iteration 85: Merging pair ('Th', 'e') with frequency 8\n",
      "Iteration 86: Merging pair ('mo', 'del') with frequency 8\n",
      "Iteration 87: Merging pair ('model', 'ing') with frequency 8\n",
      "Iteration 88: Merging pair ('p', 'e') with frequency 8\n",
      "Iteration 89: Merging pair ('w', 'i') with frequency 8\n",
      "Iteration 90: Merging pair ('wi', 'll') with frequency 8\n",
      "Iteration 91: Merging pair ('A', 'ss') with frequency 8\n",
      "Iteration 92: Merging pair ('Ass', 'ignment') with frequency 8\n",
      "Iteration 93: Merging pair ('O', 'ct') with frequency 8\n",
      "Iteration 94: Merging pair ('No', 'v') with frequency 8\n",
      "Iteration 95: Merging pair ('c', 'ourse') with frequency 7\n",
      "Iteration 96: Merging pair ('s', 'u') with frequency 7\n",
      "Iteration 97: Merging pair ('i', 'd') with frequency 7\n",
      "Iteration 98: Merging pair ('o', 'm') with frequency 7\n",
      "Iteration 99: Merging pair ('rs', 't') with frequency 7\n",
      "Iteration 100: Merging pair ('1', ':') with frequency 7\n",
      "\n",
      "Final tokenized output:\n",
      "{AI}R6001 {AI} {and} Ap{plications} / MDS6{10}5 Adv{an}{ce}d {AI}\n",
      "\n",
      "{AI}R6001 {AI} {and} Ap{plications}\n",
      "MDS6{10}5 Adv{an}{ce}d {AI}\n",
      "\n",
      "Spr{ing} 2023\n",
      "\n",
      "{Th}{is} {course} {is} {mo}{re} {re}lev{an}t {th}{an} e{ve}r giv{en} {the} {re}c{ent} {su}r{ge} {in} Art{if}{ici}{al} {In}{tell}{ig}{en}{ce} t{ec}h{no}{log}i{es}.\n",
      "It {in}t{ro}du{ce}s {the} f{un}d{am}{ent}{al} c{on}{ce}{pt}s, h{is}t{or}y, {and} adv{an}{ce}d t{ec}h{no}{log}i{es} {of} {AI} {in} v{ar}i{ou}s {ap}{plications}, \n",
      "p{ro}v{id}{ing} s{tu}d{ent}s w{it}h a c{om}p{re}h{en}si{ve} {un}{de}{rst}{and}{ing} {of} {th}{is} r{ap}{id}ly evolv{ing} field. \n",
      "{The} {course} co{ve}{rs} b{as}{ic} c{on}{ce}{pt}s {of} {AI}, {su}{ch} {as} {in}{tell}{ig}{ent} ag{ent}s, p{ro}blem-solv{ing}, \n",
      "s{e{ar}}{ch}, {and} fi{rst}-{or}{de}r {log}{ic}. It {al}so {del}v{es} {in}{to} f{un}d{am}{ent}{al} {AI} t{ec}h{no}{log}i{es}, {in}clud{ing} {re}g{re}{ss}{ion},\n",
      "p{at}{ter}n {re}c{og}n{it}{ion}, sequ{ent}i{al} {modeling}, d{at}a m{in}{ing}, {de}ep l{e{ar}n}{ing}, {and} {su}{pe}rv{is}{ed} {modeling}. \n",
      "Add{it}{ion}{al}ly, {the} {course} {ex}{pl}o{re}s t{ec}h{no}{log}i{es} {us}{ed} {in} v{ar}i{ou}s {AI} {ap}{plications}, {su}{ch} {as} {an}{om}{al}y {de}t{ec}t{ion}, \n",
      "{ed}{ge} {AI}, i{ma}{ge} p{ro}{ce}{ss}{ing}, s{pe}{ec}h p{ro}{ce}{ss}{ing}, n{at}ur{al} l{an}gua{ge} p{ro}{ce}{ss}{ing} ({in}clud{ing} {ma}{ch}{in}e tr{an}sl{ation} \n",
      "{and} di{al}{og}ue {modeling}), sci{ent}{if}{ic} p{ap}er {an}{al}ys{is}, {AI} {in} h{ea}l{th}ca{re}, aut{on}{om}{ou}s driv{ing}, {and} DNN {in}{ter}p{re}tabil{it}y {and} bi{as}.\n",
      "\n",
      "T{each}{ing} t{ea}m\n",
      "\n",
      "{In}stru{ct}{or}  S{at}oshi Nak{am}ura\n",
      "\n",
      "TA  Xi Ch{en}\n",
      "\n",
      "L{og}{is}t{ic}s\n",
      "\n",
      "{Le{ct}ure}s: a{re} {on} Tu{es}day/{Th}u{rs}day {1:}30PM - {3:}00PM {in} TXB201(TBD). {No}{te}: l{e{ct}ure}s {will} be {re}{mo}{te}\n",
      "{for} {the} fi{rst} two weeks, {and} hybr{id} af{ter}w{ar}ds. {The} Zo{om} l{in}k {is} pos{te}d {on} BB.\n",
      "Off{ic}e h{ours} \n",
      "\n",
      "P{ro}f. Nak{am}ura: {Th}u 2:30-{3:}30 PM. T{each}{ing} C{om}{pl}{ex} 704\n",
      "TA. Xi Ch{en}: W{ed} 7-9PM. T{each}{ing} C{om}{pl}{ex} 703, {Se}{at} {No}.7\n",
      "\n",
      "C{on}ta{ct}: If y{ou} ha{ve} {an}y qu{es}t{ion}, {pl}{ea}se {re}a{ch} {ou}t {to} {us} via e{ma}il {or} post {it} {to} BB.\n",
      "\n",
      "C{ourse} {In}{for}m{ation}\n",
      "\n",
      "{Th}{is} {course} {is} {de}s{ign}{ed} {for} s{tu}d{ent}s {in}{te}{re}s{te}d {in} {ar}t{if}{ici}{al} {in}{tell}{ig}{en}{ce}. {The} fi{rst} h{al}f {ma}{in}ly co{ve}{rs} \n",
      "{the} f{un}d{am}{ent}{al}s {of} {ar}t{if}{ici}{al} {in}{tell}{ig}{en}{ce}, {and} {the} s{ec}{on}d h{al}f foc{us}{es} {on} {the} {ap}{plications} {of} {ar}t{if}{ici}{al} \n",
      "{in}{tell}{ig}{en}{ce} {in} v{ar}i{ou}s fields.\n",
      "\n",
      "{In} p{ar}t{ic}ul{ar}, {the} {to}p{ic}s {in}clu{de}:\n",
      "\n",
      "{AI} H{is}t{or}y\n",
      "{In}{tell}{ig}{ent} Ag{ent}s\n",
      "Deep L{e{ar}n}{ing}\n",
      "I{ma}{ge} P{ro}{ce}{ss}{ing}\n",
      "N{at}ur{al} L{an}gua{ge} P{ro}{ce}{ss}{ing}\n",
      "S{pe}{ec}h P{ro}{ce}{ss}{ing}\n",
      "Di{al}{log}ue\n",
      "{AI} {ap}{plications} {in} H{ea}l{th} {and} {Se}lf-driv{ing} C{ar}, etc.\n",
      "\n",
      "P{re}{re}qu{is}{it}{es}\n",
      "\n",
      "P{ro}f{ici}{en}cy {in} LaT{ex}:  A{ll} {the} {re}p{or}ts ne{ed} {to} be wr{it}t{en} by {us}{ing} LaT{ex}. A {te}m{pl}{at}e {will} be\n",
      "p{ro}vi{de}d. If y{ou} a{re} {no}t f{am}ili{ar} w{it}h LaT{ex}, {pl}{ea}se l{e{ar}n} f{ro}m {the} {tu}t{or}i{al} {in} adv{an}{ce}.\n",
      "\n",
      "P{ro}f{ici}{en}cy {in} G{it}Hub:  A{ll} {the} s{ou}r{ce} co{de}s ne{ed} {to} be {su}bm{it}{te}d {in} G{it}Hub. \n",
      "P{ro}f{ici}{en}cy {in} Py{th}{on}: A{ll} {the} a{ss}{ignment}s {will} be {in} Py{th}{on} ({us}{ing} Numpy {and} PyT{or}{ch}). \n",
      "B{as}{ic} {ma}{ch}{in}e l{e{ar}n}{ing} k{no}wl{ed}{ge}:  It {is} po{ss}ible {to} take {th}{is} {course} w{it}h{ou}t {an}y {ma}{ch}{in}e\n",
      "l{e{ar}n}{ing} k{no}wl{ed}{ge}, howe{ve}r, {the} {course} {will} be {ea}sier {if} y{ou} ha{ve} f{ou}nd{ations} {of} {ma}{ch}{in}e l{e{ar}n}{ing}.\n",
      "\n",
      "B{as}{ic} C{on}{ce}{pt}s {of} p{ro}babil{it}y:  It {will} be {ea}sier {for} y{ou} {to} {un}{de}{rst}{and} s{om}e l{e{ct}ure}s {if} y{ou}\n",
      "k{no}w b{as}{ic}s {of} p{ro}babil{it}y. \n",
      "\n",
      "T{ex}tbooks\n",
      "\n",
      "R{ec}{om}m{en}{de}d Books:\n",
      "\n",
      "Art{if}{ici}{al} {In}{tell}{ig}{en}{ce}: A Mo{de}rn App{ro}a{ch} 4{th} Ed{it}{ion}\n",
      ", by S{tu}{ar}t Ru{ss}e{ll} {and} Pe{ter} N{or}v{ig}\n",
      "\n",
      "Grad{ing} Pol{ic}y ({AI}R6001/MDS6{10}5)\n",
      "{Assignment}s (30%)\n",
      "\n",
      "{Assignment} 1 ({10}%): {Le{ct}ure} 1 - {Le{ct}ure} 6\n",
      "{Assignment} 2 ({10}%): {Le{ct}ure} 7 - {Le{ct}ure} 11\n",
      "{Assignment} 3 ({10}%): {Le{ct}ure} 12 - {Le{ct}ure} 26\n",
      "\n",
      "M{id}{ter}m {ex}{am} (30%)\n",
      "(TBD) We {will} ha{ve} a m{id}-{ter}m {ex}{am} af{ter} l{e{ct}ure} 14. {The} sco{pe} {of} {the} m{id}-{ter}m {ex}{am} {is} f{ro}m l{e{ct}ure} 1 {to}\n",
      "l{e{ct}ure} 14. \n",
      "\n",
      "F{in}{al} {ex}{am} (35%)\n",
      "(TBD) We {will} ha{ve} a f{in}{al} {ex}{am} {on} TBD. {The} sco{pe} {of} {the} f{in}{al} {ex}{am} {is} f{ro}m l{e{ct}ure} 1 {to} l{e{ct}ure} 28.\n",
      "\n",
      "P{ar}t{ici}p{ation} (5%)\n",
      "(TBD)He{re} a{re} s{om}e ways {to} {e{ar}n} {the} p{ar}t{ici}p{ation} c{re}d{it}, wh{ic}h {is} c{ap}p{ed} {at} 5%. \n",
      "\n",
      "C{ourse} {and} T{each}{ing} Ev{al}u{ation} (CTE): {The} s{ch}ool {will} s{en}d {re}qu{es}ts {for} CTE {to} {al}l s{tu}d{ent}s.\n",
      "{The} CTE {is} w{or}{th} 1% c{re}d{it}.\n",
      "\n",
      "L{at}e Pol{ic}y\n",
      "{The} p{en}{al}ty {is} 0.5% {of}f {the} f{in}{al} {course} gra{de} {for} {each} l{at}e day.\n",
      "\n",
      "S{ch}{ed}ule\n",
      "\n",
      "D{at}e\n",
      "{Le{ct}ure} D{es}cri{pt}{ion}\n",
      "R{ea}d{ing}s\n",
      "{Le{ct}ure} {No}{te}\n",
      "Ev{ent}s/D{ea}dl{in}{es}\n",
      "\n",
      "{Se}{pt} 3\n",
      "{Le{ct}ure} {1:} C{ourse} {In}t{ro}du{ct}{ion}/{AI} {In}t{ro}du{ct}{ion}\n",
      "\n",
      "[Sli{de}s]\n",
      "\n",
      "{Se}{pt} 5\n",
      "{Le{ct}ure} 2: {AI} h{is}t{or}y/{In}{te}l{ig}{ent} ag{ent}\n",
      "\n",
      "[Sli{de}s]\n",
      "\n",
      "{Se}{pt} {10}\n",
      "{Le{ct}ure} {3:} {AI} Solv{ing} P{ro}blems by S{e{ar}}{ch}{ing}\n",
      "\n",
      "{Se}{pt} 12\n",
      "{Le{ct}ure} 4: {AI} {In}{for}m{ed} S{e{ar}}{ch}\n",
      "\n",
      "{Se}{pt} 19\n",
      "{Le{ct}ure} 5: {AI} {in} C{on}stra{in}t s{at}{is}fa{ct}{ion} P{ro}blems\n",
      "\n",
      "{Se}{pt} 24\n",
      "{Le{ct}ure} 6: {AI} {in} L{og}{ic}{al} Ag{ent} {and} 1st {or}{de}r {log}{ic}\n",
      "\n",
      "{Assignment} 1 {ou}t \n",
      "\n",
      "{Se}{pt} 26\n",
      "{Le{ct}ure} 7: {AI} Reg{re}{ss}{ion}\n",
      "\n",
      "{Oct} 8\n",
      "{Le{ct}ure} 8: {AI} P{at}{ter}n R{ec}{og}n{it}{ion}\n",
      "\n",
      "{Oct} {10}\n",
      "{Le{ct}ure} 9: {AI} {Se}qu{en}{ce} {modeling}\n",
      "\n",
      "{Oct} 15\n",
      "{Le{ct}ure} {10}: {AI} {in} D{at}a m{in}{ing}\n",
      "\n",
      "{Assignment} 2 {ou}t \n",
      "\n",
      "{Oct} 17\n",
      "{Le{ct}ure} 1{1:} {AI} {in} Deep L{e{ar}n}{ing} F{un}d{am}{ent}{al}s 1\n",
      "\n",
      "{Oct} 22\n",
      "{Le{ct}ure} 12: {AI} {in} Deep L{e{ar}n}{ing} F{un}d{am}{ent}{al}s 2\n",
      "\n",
      "{Oct} 24\n",
      "{Le{ct}ure} 1{3:} {AI} {Se}lf-{su}{pe}rv{is}{ed} {modeling}\n",
      "\n",
      "{Oct} 29\n",
      "M{id}{ter}m {ex}{am}\n",
      "\n",
      "{Oct} 31\n",
      "{Le{ct}ure} 14: {AI} {in} An{or}m{al}y 1\n",
      "\n",
      "{Assignment} 3 {ou}t \n",
      "\n",
      "{Nov} 5\n",
      "{Le{ct}ure} 15: {AI} {in} An{or}m{al}y 2\n",
      "\n",
      "{Nov} 7\n",
      "{Le{ct}ure} 16: Ed{ge} {AI}\n",
      "\n",
      "{Nov} 12\n",
      "{Le{ct}ure} 17: {AI} I{ma}{ge} P{ro}{ce}{ss}{ing}\n",
      "\n",
      "{Nov} 14\n",
      "{Le{ct}ure} 18: {AI} {in} S{pe}{ec}h P{ro}{ce}{ss}{ing}\n",
      "\n",
      "{Nov} 19\n",
      "{Le{ct}ure} 19: {AI} {in} N{at}ur{al} L{an}gua{ge} P{ro}{ce}{ss}{ing}\n",
      "\n",
      "{Nov} 21\n",
      "{Le{ct}ure} 20: {AI} {in} Ma{ch}{in}e Tr{an}sl{ation} 1\n",
      "\n",
      "{Nov} 26\n",
      "{Le{ct}ure} 2{1:} {AI} {in} Ma{ch}{in}e Tr{an}sl{ation} 2\n",
      "\n",
      "{Nov} 28\n",
      "{Le{ct}ure} 22: {AI} {in} Di{al}{og}ue Sys{te}m\n",
      "\n",
      "D{ec} 3\n",
      "{no} cla{ss} (SLT2024)\n",
      "\n",
      "D{ec} 5\n",
      "{Le{ct}ure} 2{3:} {AI} {in} H{ea}l{th}, M{ed}{ic}{al} I{ma}{ge} P{ro}{ce}{ss}{ing} I\n",
      "\n",
      "D{ec} {10}\n",
      "{Le{ct}ure} 24: {AI} {in} {Se}lf-driv{ing} C{ar}\n",
      "\n",
      "D{ec} 12\n",
      "{Le{ct}ure} 25: {AI} {in} {In}{ter}p{re}tabil{it}y {and} E{th}{ic}s\n"
     ]
    }
   ],
   "source": [
    "import collections\n",
    "\n",
    "# sub-word BPE algorithm\n",
    "def get_stats(vocab):\n",
    "    # calculate most frequency char pairs\n",
    "    pairs = collections.defaultdict(int)\n",
    "    for word, freq in vocab.items():\n",
    "        symbols = word.split()\n",
    "        for i in range(len(symbols) - 1):\n",
    "            pairs[(symbols[i], symbols[i + 1])] += freq\n",
    "    return pairs\n",
    "\n",
    "def merge_vocab(pair, vocab):\n",
    "    # merge most frequent char pairs\n",
    "    bigram = ' '.join(pair)\n",
    "    new_vocab = {}\n",
    "    for word in vocab:\n",
    "        new_word = word.replace(bigram, ''.join(pair))\n",
    "        new_vocab[new_word] = vocab[word]\n",
    "    return new_vocab\n",
    "\n",
    "def bpe(vocab, num_merges):\n",
    "    # do k times of merge \n",
    "    merges = []\n",
    "    for i in range(num_merges):\n",
    "        pairs = get_stats(vocab)\n",
    "        if not pairs:\n",
    "            break\n",
    "        # find the most frequent char pairs\n",
    "        best = max(pairs, key=pairs.get)\n",
    "        merges.append(best)\n",
    "        print(f\"Iteration {i + 1}: Merging pair {best} with frequency {pairs[best]}\")\n",
    "        vocab = merge_vocab(best, vocab)\n",
    "    return vocab, merges\n",
    "\n",
    "# enter the file path\n",
    "file_path = '/Users/mengrui/Desktop/output.txt'  \n",
    "\n",
    "# read the file content\n",
    "with open(file_path, 'r') as file:\n",
    "    text = file.read().strip()\n",
    "# create a vocab, seperate every char with a space in case to make merge more convenient\n",
    "vocab = {' '.join(word): text.count(word) for word in text.split()}\n",
    "\n",
    "# do the BPE algorithm with 20 times\n",
    "final_vocab, merges = bpe(vocab, num_merges=100)\n",
    "\n",
    "# print out the final form of token\n",
    "output = list(text)\n",
    "for merge in merges:\n",
    "    tokens_to_merge = ['{' + token + '}' if len(token) > 1 else token for token in merge]\n",
    "    replaced_token = ''.join(tokens_to_merge)\n",
    "    bigram = ''.join(merge)\n",
    "    new_token = '{' + bigram + '}'\n",
    "    text_str = ''.join(output)\n",
    "    text_str = text_str.replace(replaced_token, new_token)\n",
    "    output = list(text_str)\n",
    "\n",
    "final_output = ''.join(output)\n",
    "\n",
    "print(\"\\nFinal tokenized output:\")\n",
    "print(final_output)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After 100 times of interation, the tokens in the text are quite likely to be in a word form, for exemple, AI, in, Lecture, etc. As the time of interations increase, frequently mentioned char pairs merged into together, and they are more likely to be a specific part of word. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "test_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

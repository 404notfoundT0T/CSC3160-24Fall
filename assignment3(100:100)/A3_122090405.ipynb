{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Task 1: Language Models [30 marks]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Implement an N-gram language model (e.g., bigram or trigram) using the four provided text files (country.txt, pop.txt, rap.txt, and rock.txt).\n",
    "2. Predict the next word of each n-gram of the words in Sentence 1 using your N-gram model. For example, what is the predicted next word given two words “Tonight I” using the trigram model. You can answer this question in the following format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: ('tonight', 'i') --- Output: 'm\n",
      "Input: ('i', 'will') --- Output: follow\n",
      "Input: ('will', 'make') --- Output: it\n",
      "Input: ('make', 'the') --- Output: world\n",
      "Input: ('the', 'evening') --- Output: dump\n",
      "Input: ('evening', 'meal') --- Output: >\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.util import ngrams\n",
    "from collections import defaultdict, Counter\n",
    "\n",
    "files = [\"/Users/mengrui/Desktop/A3/country.txt\", \"/Users/mengrui/Desktop/A3/pop.txt\", \"/users/mengrui/Desktop/A3/rap.txt\", \"/Users/mengrui/Desktop/A3/rock.txt\"]\n",
    "\n",
    "def load_data(file_paths):\n",
    "    text = \"\"\n",
    "    for file_path in file_paths:\n",
    "        with open(file_path, 'r') as file:\n",
    "            text += file.read().lower() + \" \"  \n",
    "    return text\n",
    "\n",
    "text = load_data(files)\n",
    "tokens = nltk.word_tokenize(text)\n",
    "\n",
    "N = 3\n",
    "\n",
    "ngrams_list = list(ngrams(tokens, N, pad_left=True, pad_right=True))\n",
    "n_minus_1_grams_list = list(ngrams(tokens, N-1, pad_left=True, pad_right=True))\n",
    "\n",
    "n_minus_1_grams_counts = Counter(n_minus_1_grams_list)\n",
    "n_grams_counts = Counter(ngrams_list)\n",
    "\n",
    "vocab_size = len(set(tokens))\n",
    "\n",
    "conditional_probs = defaultdict(lambda: defaultdict(float))\n",
    "\n",
    "for ngram in n_grams_counts:\n",
    "    n_minus_1_gram = ngram[:-1]\n",
    "    word = ngram[-1]\n",
    "    conditional_probs[n_minus_1_gram][word] = (n_grams_counts[ngram] + 1) / (n_minus_1_grams_counts[n_minus_1_gram] + vocab_size)\n",
    "\n",
    "def predict_next_word(context):\n",
    "    context = tuple(context[-(N-1):])  \n",
    "    if context in conditional_probs:\n",
    "        possible_words = conditional_probs[context]\n",
    "        return max(possible_words, key=possible_words.get)\n",
    "    else:\n",
    "        return max(n_grams_counts, key=n_grams_counts.get)[-1] if n_grams_counts else None\n",
    "\n",
    "sentence = \"Tonight I will make the evening meal.\"  \n",
    "sentence_tokens = nltk.word_tokenize(sentence.lower())\n",
    "\n",
    "for i in range(len(sentence_tokens) - (N-1)):\n",
    "    context = sentence_tokens[i:i + (N-1)]\n",
    "    next_word = predict_next_word(context)\n",
    "    print(f\"Input: {tuple(context)} --- Output: {next_word}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Task 2: Word Embeddings [70 marks]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Use a pretrained word2vec model to conduct some experiments. Please refer to the hints12. Please load the pretrained word2vec model with the following codes.\n",
    "     !pip -qq install gensim\n",
    "     import gensim.downloader as api\n",
    "     model = api.load(’word2vec-google-news-300’)\n",
    "(a) Get the five most similar words to “speech”."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word: speeches, Similarity: 0.6758114099502563\n",
      "Word: keynote_speech, Similarity: 0.6579364538192749\n",
      "Word: speach, Similarity: 0.6468180418014526\n",
      "Word: remarks, Similarity: 0.6410110592842102\n",
      "Word: Speech, Similarity: 0.6331154704093933\n"
     ]
    }
   ],
   "source": [
    "import gensim.downloader as api\n",
    "\n",
    "model=api.load('word2vec-google-news-300')\n",
    "\n",
    "similar_words = model.similar_by_word(\"speech\", topn=5)\n",
    "for word, similarity in similar_words:\n",
    "    print(f\"Word: {word}, Similarity: {similarity}\")\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(b) Confirm that “queen′′ = “king′′ − “male′′ + “f emale′′ (“queen′′ should be the three most similar words of the right-hand equation.)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To prove the similarity between \"queen\" and \"king\"-\"male\"+\"female\", we first set a vector of \"king\"-\"male\"+\"female\", then we use gensim.models.Word2Vec.similar_by_vector func to find the three most familiar words with \"king\"-\"male\"+\"female\"vector, and in the out put there is \"queen\", thus we can confirm that( “queen′′ should be the three most similar words of the right-hand equation.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word: king, Similarity: 0.8830681443214417\n",
      "Word: queen, Similarity: 0.6669612526893616\n",
      "Word: kings, Similarity: 0.6140398979187012\n"
     ]
    }
   ],
   "source": [
    "import gensim.downloader as api\n",
    "\n",
    "model=api.load('word2vec-google-news-300')\n",
    "\n",
    "result_vector = model['king'] - model['male'] + model['female']\n",
    "\n",
    "similar_words = model.similar_by_vector(result_vector, topn=3)\n",
    "\n",
    "for word, similarity in similar_words:\n",
    "    print(f\"Word: {word}, Similarity: {similarity}\")\n",
    "\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculate the similarity of two sentences using the word2vec model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentence Similarity: 0.773637056350708\n"
     ]
    }
   ],
   "source": [
    "from gensim.models import KeyedVectors\n",
    "import gensim.downloader as api\n",
    "\n",
    "model = api.load('word2vec-google-news-300')\n",
    "\n",
    "sentence1 = \"Tonight, I will make the evening meal.\"\n",
    "sentence2 = \"I am going to make dinner tonight.\"\n",
    "\n",
    "words1 = sentence1.lower().split()\n",
    "words2 = sentence2.lower().split()\n",
    "\n",
    "similarity_score = model.n_similarity(words1, words2)\n",
    "print(f\"Sentence Similarity: {similarity_score}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. The function below is called Jaccard similarity. Explain how Jaccard similarity computes the similarity of sentences in a few sentences. And, calculate the Jaccard similarity of Sentence 1 and Sentence 2."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First tokenize two sentences and convert the words to lowercase, then we have two sets of words. Then we calculate intersection, which contains the words both in two sets. And also we calculate union, which contains all the words present in either set. Finally we calculate the similarity by dividing the size of intersection by the size of union."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Jaccard Similarity: 0.16666666666666666\n"
     ]
    }
   ],
   "source": [
    "sentence1 = \"Tonight, I will make the evening meal.\"\n",
    "sentence2 = \"I am going to make dinner tonight.\"\n",
    "\n",
    "def jaccard_similarity(sentence1, sentence2):\n",
    "    tokens1 = set(sentence1.lower().split())\n",
    "    tokens2 = set(sentence2.lower().split())\n",
    "    intersection = tokens1.intersection(tokens2)\n",
    "    union = tokens1.union(tokens2)\n",
    "    return len(intersection) / len(union)\n",
    "\n",
    "similarity_score = jaccard_similarity(sentence1, sentence2)\n",
    "print(f\"Jaccard Similarity: {similarity_score}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. Why do the similarity scores of word2vec and Jaccard similarity differ a lot?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In my opion, it's mainly because the underlying machanism of two models are different. Word2Vec model capture semantic similarity based on the context in which words appear in larger corpora. And the vector representation of words are dense. Thus Word2Vec model can recognize synoyms and semantically related words. In contrast, Jaccard similarity is a simple metric which solely rely on presence or absence of words in the sets derived from the sentences. It does not consider word meanings or relationships."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "test_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
